{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data from API with Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create Spark session (a Spark Context is automatically created when the spark job runs)\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Specify the API endpoint and parameters\n",
        "url = \"https://api.openweathermap.org/data/2.5/forecast\"\n",
        "params = {\"q\": \"London,uk\", \"appid\": \"<api-key>\"}\n",
        "\n",
        "# Make the API request and parse the response as json\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Spark principles\n",
        "At a high level, every Spark application consists of a driver program that runs the userâ€™s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a RDD\n",
        "rddjson = sc.parallelize([data])\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.read.json(rddjson)\n",
        "\n",
        "# View the DF\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We could create the dataframe from a sublevel of the json\n",
        "        \n",
        "        rddjson1 = sc.parallelize([data[\"list\"]])\n",
        "        df1 = spark.read.json(rddjson1)\n",
        "        df1.show()\n",
        "\n",
        "You can try this but for this practice we will not use this"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write file from dataframe (options)\n",
        "\n",
        "Option 3 is the one we will use in this practice. The first two, are for you to explore behavior. You can view the resulting files in the data folder for this session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Option 1: directly from spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set datalake path\n",
        "dlpath = 'abfs://<container-name>@<datalake-name>.dfs.core.windows.net'\n",
        "\n",
        "# Write the dataframe to a json file\n",
        "df.write.format(\"json\").mode(\"overwrite\").save(dlpath + \"/raw/forecast.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Option 2: Convert to pandas from spark and then to csv\n",
        "\n",
        "As Spark works partitioning data, spark will create a directory and inside the directory will write many files with chunks of data each, and not sorted.\n",
        "So, for easier way to work with little dataframes, we can convert the Spark dataframe to a Pandas dataframe.\n",
        "Two other options are presented. There are subtle differences in the way each method converts and writes to file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to pandas --- convert to csv\n",
        "# Spark method 'toPandas' and then concatenated the Pandas method 'to_csv'\n",
        "df.toPandas().to_csv(dlpath + '/raw/toPandas_forecast.csv', index_label = False, lineterminator = '\\n', sep = ',', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Option 3: Create pandas df from original data and then to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a Pandas DataFrame from response data above in json format\n",
        "data_csv = pd.DataFrame([data])\n",
        "\n",
        "# Pandas to csv\n",
        "data_csv.to_csv(dlpath + '/raw/pd_forecast.csv', index=False)\n",
        "data_csv.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# SQL Transformations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "+ DDL statement for local sql server instance: create table and load data from csv\n",
        "\n",
        "        %%sql -- magic command: change the language of the execution for this cell\n",
        "        CREATE TABLE Weather_london_landing4_direct (\n",
        "            cod varchar(200),\n",
        "            message varchar(200),\n",
        "            cnt varchar(50),\n",
        "            list varchar(max),\n",
        "            city varchar (500)\n",
        "        ) \n",
        "        WITH (\n",
        "        LOCATION = 'path/to/pd_forecast.csv',\n",
        "            FILE_FORMAT = CSV,\n",
        "            REJECT_TYPE = VALUE,\n",
        "            REJECT_VALUE = 0,\n",
        "            FIELDQUOTE = '\"',\n",
        "            FIRSTROW = 2, -- as 1st one is header\n",
        "            FIELDTERMINATOR = ',',  --CSV field delimiter\n",
        "            ROWTERMINATOR = '\\n'   --Use to shift the control to next row\n",
        "        );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "+ Create a local final(cleaned) table in the sql pool\n",
        "\n",
        "      %%sql\n",
        "      CREATE TABLE Weather_london (\n",
        "        dt int,\n",
        "        dt_txt datetime,\n",
        "        cloudiness int,\n",
        "        precip_prob float,\n",
        "        humidity float,\n",
        "        \"temp\" float,\n",
        "        temp_max float,\n",
        "        temp_min float,\n",
        "        visibility float,\n",
        "        wind_speed float\n",
        "      );\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "+ TRANSFORM 1: Change single quote inside list field to doble quotes\n",
        "\n",
        "        %%sql\n",
        "        UPDATE Weather_london_landing\n",
        "        SET 'list' = REPLACE(list, char(39),'\"');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "+ TRANSFORM 2 : Extract from JSON, convert data types, insert into final table\n",
        "\n",
        "        %%sql\n",
        "        DECLARE @json NVARCHAR(MAX);\n",
        "        SELECT @json = list FROM Weather_london_landing    -- Set list field as json object and insert into\n",
        "        INSERT INTO Weather_london\n",
        "        SELECT *\n",
        "        FROM OPENJSON ( @json )  \n",
        "        WITH (   \n",
        "            dt  int                 '$.dt',  \n",
        "            dt_txt  datetime        '$.dt_txt',   \n",
        "            cloudiness int          '$.clouds.all',\n",
        "            precip_prob float       '$.pop',\n",
        "            temp float              '$.main.temp',\n",
        "            temp_max float          '$.main.temp_max',\n",
        "            temp_min float          '$.main.temp_min',\n",
        "            humidity int            '$.main.humidity', \n",
        "            visibility float        '$.visibility',\n",
        "            wind_speed float        '$.wind.speed'\n",
        "        );"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
